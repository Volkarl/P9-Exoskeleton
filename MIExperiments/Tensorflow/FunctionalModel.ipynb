{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense, Conv1D, concatenate, Flatten\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "seed = 345045234\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [],
   "source": [
    "file_path = \"..\\\\..\\\\Datasets\\\\BigData.xlsx\"\n",
    "data_title = \"data_test_raw_data\"\n",
    "\n",
    "columns_data = ['1' ,'2', '3', '4', '5', '6', '7', '8', 'N/A_1', 'N/A_2', 'angle', 'time', 'session']\n",
    "columns_features_considered = columns_data[:8]\n",
    "column_ground_truth = columns_data[10]\n",
    "# Note that we ignore the 'time' column. That makes our data slightly imprecise as there are tiny, \n",
    "# TINY differences in time intervals in the real data (not worth modeling). Each timestep represents \n",
    "# 1 millisecond, 0.001 second. \n",
    "\n",
    "# TODO: Change index columns to seconds instead of milliseconds. Perhaps lambda round all time measurements, then use \n",
    "# as the index? Or just give a range(0, len(array) * 0.001, 0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [],
   "source": [
    "# Read first sheet (table of contents), find index of entry with correct title, then load the corresponding excel sheet\n",
    "table_of_contents = pd.read_excel(file_path, sheet_name=0, header=None)\n",
    "sheet_index = table_of_contents[table_of_contents[0] == data_title][0].index[0]\n",
    "sheet_data = pd.read_excel(file_path, sheet_name=sheet_index + 1, header=None)\n",
    "sheet_data.columns = columns_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%# Load data\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [],
   "source": [
    "features = sheet_data[columns_features_considered]\n",
    "# features.index = sheet_data['time'] \n",
    "# If I index time, then we have floats as indexes. One row already represents one time point\n",
    "\n",
    "ground_truth = pd.DataFrame(sheet_data[column_ground_truth])\n",
    "#ground_truth.index = sheet_data['time']\n",
    "\n",
    "#data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\n",
    "#data_std = dataset[:TRAIN_SPLIT].std(axis=0)\n",
    "#dataset = (dataset-data_mean)/data_std\n",
    "# SEEMS UNECESSARY, AS WE NORMALIZE ON THE SENSOR\n",
    "# HOW MUCH DIFFERENCE IS THERE?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Split data\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "features.plot(subplots=True)\n",
    "\n",
    "ground_truth.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Plot data\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [],
   "source": [
    "def multivariate_data(dataset_features, dataset_ground_truth, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False, print_index=False):\n",
    "  data, labels, history = [], [], []\n",
    "\n",
    "  start_index = start_index + history_size \n",
    "  # such that we always have history_size (100) observations to base our predictions on\n",
    "  if end_index is None:\n",
    "    end_index = len(dataset) - target_size \n",
    "    # such that we never predict based on the last future_target (10) measurements\n",
    "\n",
    "  if print_index: print(\"start\")\n",
    "  for i in range(start_index, end_index): # start 100, end 790. \n",
    "      # TODO: It actually goes to 800 (when end_index is not None), meaning that our last 10 predictions cannot be verified with our training set. This is a problem\n",
    "    if print_index: print(\"A\", i,)\n",
    "    indices = range(i-history_size, i, step) # range(0, 100) step size of 1          --- our sliding window\n",
    "    data.append(dataset_features[indices]) # append new array that contains all values within our sliding window\n",
    "    # TODO: ONE PROBLEM KINDA? Step size makes no sense. Doesn't feel like there's any point to it. \n",
    "    history.append(dataset_ground_truth[indices])\n",
    "\n",
    "    if single_step:\n",
    "      labels.append(dataset_ground_truth[i+target_size])\n",
    "    else:\n",
    "      labels.append(dataset_ground_truth[i:i+target_size])\n",
    "\n",
    "  return np.array(data), np.array(labels), np.array(history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Create array of all sliding windows of the data\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [],
   "source": [
    "STEP = 10\n",
    "past_history = 1000\n",
    "future_target = 1\n",
    "# Aggregates STEP predictions, looks at a total of PASTHISTORY observations, \n",
    "# predicts the observation at FUTURETARGET observations in the future\n",
    "dataset = features.values\n",
    "ground_truth2 = ground_truth.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% MI Defines\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [],
   "source": [
    "x_train, y_train, _ = multivariate_data(dataset, ground_truth2, 0,\n",
    "                                                   None, past_history,\n",
    "                                                   future_target, STEP,\n",
    "                                                   single_step=False, print_index = False)\n",
    "\n",
    "print ('Single window of past history : {}'.format(x_train[0].shape))\n",
    "print(len(x_train), len(y_train))\n",
    "print(x_train.shape)\n",
    "y_train = np.squeeze(y_train, axis=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Get training data sliding windows\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [
    "#input\n",
    "inp = int(past_history / STEP)\n",
    "\n",
    "inputs = Input(shape=(inp,8), name='main_input')\n",
    "\n",
    "conv = Conv1D(filters=50,kernel_size=int(inp/10),strides=2,padding='valid',activation='relu')(inputs)\n",
    "\n",
    "conv = Conv1D(filters=20,kernel_size=4,strides=2,padding='valid',activation='relu')(conv)\n",
    "\n",
    "conv = Conv1D(filters=10,kernel_size=4,strides=2,padding='valid',activation='relu')(conv)\n",
    "\n",
    "lstm = LSTM(32)(inputs)\n",
    "\n",
    "x = Flatten()(conv)\n",
    "\n",
    "dense_1 = Dense(32,activation='relu',name='dense1')(x)\n",
    "\n",
    "dense_2 = Dense(32, activation='relu',name='dense2')(lstm)\n",
    "\n",
    "auxiliary_output_1 = Dense(1, activation='sigmoid', name='aux_output1')(x)\n",
    "\n",
    "auxiliary_output_2 = Dense(1, activation='sigmoid', name='aux_output2')(lstm)\n",
    "\n",
    "print(\"success\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [],
   "source": [
    "#intermediate layers\n",
    "\n",
    "\n",
    "concat = concatenate([dense_1,dense_2])\n",
    "\n",
    "x = Dense(128, activation='relu')(concat)\n",
    "x = Dense(96, activation='relu')(x)\n",
    "x = Dense(72, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "main_output = Dense(1, activation='sigmoid',name='main_output')(x)\n",
    "\n",
    "print('success')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[main_output,auxiliary_output_1,auxiliary_output_2])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.01), loss={'main_output':'mae',\n",
    "                                                                          'aux_output1':'mae',\n",
    "                                                                          'aux_output2':'mae'},\n",
    "              loss_weights={'main_output':1, 'aux_output1':0.3, 'aux_output2':0.2})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [],
   "source": [
    "history = model.fit({'main_input': x_train},\n",
    "          {'main_output': y_train, 'aux_output1': y_train, 'aux_output2': y_train},\n",
    "          epochs=20, batch_size=BATCH_SIZE,validation_split=0.2)\n",
    "print(\"succ\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [],
   "source": [
    "print(\"start\")\n",
    "test_score = model.evaluate({'main_input': x_train},\n",
    "          {'main_output': y_train, 'aux_output1': y_train, 'aux_output2': y_train},verbose=1,batch_size=1000)\n",
    "\n",
    "print('Test loss:', test_score[0])\n",
    "print('Test accuracy:', test_score[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [],
   "source": [
    "def plot_train_history(history, title):\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  epochs = range(len(loss))\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "  plt.title(title)\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "plot_train_history(history, 'Multi-Step Training and validation loss')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}