{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dataset(DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY, SMOOTHING, MEAN, VAL_PERCENT, PAST_HISTORY, \n",
    "                  FUTURE_TARGET, STEP_SIZE_SLIDING_WINDOW, message, USE_REF_POINTS, REF_POINT1, REF_POINT2):\n",
    "    print(f\"Working on dataset: {DATASET_FILE_PATH} {DATASET_SHEET_TITLE} {message}\")\n",
    "    raw_data = load_dataset(DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY)\n",
    "    indexes, features, ground_truth = split_data(raw_data, GRANULARITY, SMOOTHING, MEAN)\n",
    "    plot_dataset(features, ground_truth, indexes)\n",
    "    if(USE_REF_POINTS):\n",
    "        relative_features1 = [subtract_refvalue(obs, obs[REF_POINT1]) for obs in features.values]\n",
    "        relative_features2 = [subtract_refvalue(obs, obs[REF_POINT2]) for obs in features.values]\n",
    "        features = pd.DataFrame([relative_features1[i] + relative_features2[i] for i in range(0, len(features))])\n",
    "        plot_dataset(features, ground_truth, indexes)\n",
    "    x_train, y_train, x_val, y_val = slice_data(indexes, features, ground_truth, VAL_PERCENT, PAST_HISTORY, \n",
    "                                                FUTURE_TARGET, STEP_SIZE_SLIDING_WINDOW, GRANULARITY)\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "def subtract_refvalue(obs, ref_value):\n",
    "    return [val - ref_value for val in obs]\n",
    "\n",
    "def slice_dataset_plain(DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY, SMOOTHING, MEAN, VAL_PERCENT, PAST_HISTORY, \n",
    "                  FUTURE_TARGET, STEP_SIZE_SLIDING_WINDOW, message):\n",
    "    print(f\"Working on dataset: {DATASET_FILE_PATH} {DATASET_SHEET_TITLE} {message}\")\n",
    "    raw_data = load_dataset(DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY)\n",
    "    indexes, features, ground_truth = split_data(raw_data, GRANULARITY, SMOOTHING, MEAN)\n",
    "    # plot_dataset(features, ground_truth, indexes)\n",
    "    \n",
    "    dataset = features.values\n",
    "    observations = len(dataset)\n",
    "    val_split = int(observations * (1 - VAL_PERCENT))\n",
    "\n",
    "    x_train, y_train, x_val, y_val = features[:val_split], ground_truth[:val_split], features[val_split:], ground_truth[val_split:]\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "def process_multiple_sheets(start, end, DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY, SMOOTHING, MEAN, \n",
    "                            VAL_PERCENT, PAST_HISTORY, FUTURE_TARGET, STEP_SIZE_SLIDING_WINDOW, USE_REF_POINTS, \n",
    "                            REF_POINT1, REF_POINT2, SET_NAME):\n",
    "    x_train_combine, y_train_combine, x_val_combine, y_val_combine = [], [], [], []\n",
    "    datashape = (0,0)\n",
    "    for i in range(start, end):\n",
    "        x_train, y_train, x_val, y_val = slice_dataset(DATASET_FILE_PATH[i], DATASET_SHEET_TITLE[i], GRANULARITY, \n",
    "                                                       SMOOTHING, MEAN, VAL_PERCENT, PAST_HISTORY, FUTURE_TARGET, \n",
    "                                                       STEP_SIZE_SLIDING_WINDOW, f\"{SET_NAME} number {i}\", \n",
    "                                                       USE_REF_POINTS, REF_POINT1, REF_POINT2)\n",
    "        datashape = x_train.shape[-2:]\n",
    "        print(datashape)\n",
    "        \n",
    "        print(x_train.shape, x_val.shape)\n",
    "        x_train_combine.extend(x_train)\n",
    "        y_train_combine.extend(y_train)\n",
    "        x_val_combine.extend(x_val)\n",
    "        y_val_combine.extend(y_val)\n",
    "        print(len(x_train_combine), len(x_val_combine))\n",
    "    return x_train_combine, y_train_combine, x_val_combine, y_val_combine, datashape\n",
    "\n",
    "\n",
    "def process_data(DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY, SMOOTHING, VAL_PERCENT, \n",
    "                 PAST_HISTORY, FUTURE_TARGET, STEP_SIZE_SLIDING_WINDOW, BATCH_SIZE, EPOCHS, \n",
    "                 SHUFFLE_BUFFER_SIZE, MEAN, USE_REF_POINTS, REF_POINT1, REF_POINT2):\n",
    "    sheet_num = len(DATASET_FILE_PATH) - 1\n",
    "    \n",
    "    x_train_all, y_train_all, x_val_all, y_val_all, _ = process_multiple_sheets(0, sheet_num - 1, DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY, SMOOTHING, MEAN, VAL_PERCENT, PAST_HISTORY, FUTURE_TARGET, STEP_SIZE_SLIDING_WINDOW, USE_REF_POINTS, REF_POINT1, REF_POINT2, \"TRAIN SET\")\n",
    "    x_test_all, y_test_all, _, _, _ = process_multiple_sheets(sheet_num - 1, sheet_num, DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY, SMOOTHING, MEAN, 0, PAST_HISTORY, FUTURE_TARGET, STEP_SIZE_SLIDING_WINDOW, USE_REF_POINTS, REF_POINT1, REF_POINT2, \"TEST SET\")\n",
    "    x_plot, y_plot, _, _, datashape = process_multiple_sheets(sheet_num - 1, sheet_num, DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY, SMOOTHING, MEAN, 0, PAST_HISTORY, FUTURE_TARGET, STEP_SIZE_SLIDING_WINDOW, USE_REF_POINTS, REF_POINT1, REF_POINT2, \"PLOTTING SET\")\n",
    "    # The last one is used only for plotting, such that it doesn't take forever\n",
    "\n",
    "    batch_train_data, batch_val_data, batch_test_data, batch_plot_data = batch_data(x_train_all, y_train_all, x_val_all, y_val_all, \n",
    "                                                                   x_test_all, y_test_all, x_plot, y_plot, \n",
    "                                                                   BATCH_SIZE, EPOCHS, SHUFFLE_BUFFER_SIZE)\n",
    "    test_data_indexes = range(0, len(x_plot) * GRANULARITY, GRANULARITY) # Necessary for plotting\n",
    "    return batch_train_data, batch_val_data, batch_test_data, batch_plot_data, y_plot, len(x_train_all), len(x_val_all), test_data_indexes, datashape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions\n",
    "\n",
    "# Data layout in the xlsx files\n",
    "columns_data = ['1' ,'2', '3', '4', '5', '6', '7', '8', 'N/A_1', 'N/A_2', 'angle', 'time', 'session']\n",
    "columns_features_considered = columns_data[:8]\n",
    "column_ground_truth = columns_data[10]\n",
    "# Note that we ignore the 'time' column. That makes our data slightly imprecise as there are tiny, \n",
    "# TINY differences in time intervals in the real data (not worth modeling). Each timestep represents \n",
    "# 1 millisecond, 0.001 second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY):\n",
    "    # Read sheet 1 (table of contents), find index of entry with correct title, then load the corresponding excel sheet\n",
    "    table_of_contents = pd.read_excel(DATASET_FILE_PATH, sheet_name=0, header=None)\n",
    "    sheet_index = table_of_contents[table_of_contents[0] == f\"{DATASET_SHEET_TITLE}_raw_data\"][0].index[0]\n",
    "    sheet_data = pd.read_excel(DATASET_FILE_PATH, sheet_name=sheet_index + 1, header=None)\n",
    "    sheet_data.columns = columns_data\n",
    "    return sheet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_observations(features, indexes):\n",
    "    features_len = len(features)\n",
    "    observations_len = len(features.iloc[0])\n",
    "    df = pd.DataFrame([(sum(features.iloc[i]) / observations_len) for i in range(0, features_len)])\n",
    "    df.index = indexes\n",
    "    return df\n",
    "\n",
    "def split_data(raw_data, GRANULARITY, SMOOTHING, MEAN):\n",
    "    indexes = range(0, len(raw_data), 1)[::GRANULARITY] # Each timestep is a millisecond\n",
    "    features = raw_data[columns_features_considered][::GRANULARITY].ewm(span=SMOOTHING).mean()\n",
    "    if(MEAN): features = mean_observations(features, indexes)\n",
    "    ground_truth = pd.DataFrame(raw_data[column_ground_truth][::GRANULARITY]).ewm(span=SMOOTHING).mean()\n",
    "    return indexes, features, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(features, ground_truth, indexes):\n",
    "    features.plot(subplots=True) \n",
    "    plt.show()\n",
    "    ground_truth.plot()\n",
    "    plt.show()\n",
    "    plt.plot(indexes, features) # Show all sensors together\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of all sliding windows of the data\n",
    "def multivariate_data(dataset_features, dataset_ground_truth, start_index, end_index, history_size,\n",
    "                      target_size, step, granularity, single_step=False, print_index=False):\n",
    "    data, labels = [], []\n",
    "    start_index = start_index + history_size \n",
    "    if end_index is None:\n",
    "        end_index = len(dataset_features) - target_size \n",
    "    if print_index: print(\"start\")\n",
    "    for i in range(start_index, end_index): # start 100, end 790. \n",
    "        if print_index: print(\"A\", i,)\n",
    "        indices = range(i-history_size, i, step) # range(0, 100) step size of 1          --- our sliding window\n",
    "        data.append(dataset_features[indices]) # append new array that contains all values within our sliding window\n",
    "        if single_step:\n",
    "            labels.append(dataset_ground_truth[i+target_size])\n",
    "        else:\n",
    "            labels.append(dataset_ground_truth[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "def slice_data(indexes, features, ground_truth, VAL_PERCENT, PAST_HISTORY, FUTURE_TARGET, \n",
    "               STEP_SIZE_SLIDING_WINDOW, GRANULARITY):\n",
    "    dataset = features.values\n",
    "    observations = len(dataset)\n",
    "    val_split = int(observations * (1 - VAL_PERCENT))\n",
    "        \n",
    "    x_train, y_train = multivariate_data(dataset, ground_truth.values, 0,\n",
    "                                         val_split, PAST_HISTORY, FUTURE_TARGET, \n",
    "                                         STEP_SIZE_SLIDING_WINDOW, GRANULARITY, single_step = False, \n",
    "                                         print_index = False)\n",
    "    x_val, y_val = multivariate_data(dataset, ground_truth.values, val_split, \n",
    "                                         None, PAST_HISTORY, FUTURE_TARGET, \n",
    "                                         STEP_SIZE_SLIDING_WINDOW, GRANULARITY, single_step=False, \n",
    "                                         print_index = False)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(x_train, y_train, x_val, y_val, x_test, y_test, x_plot, y_plot, BATCH_SIZE, EPOCHS, SHUFFLE_BUFFER_SIZE):\n",
    "    batched_train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    if SHUFFLE_BUFFER_SIZE == 0:\n",
    "        batched_train_data = batched_train_data.batch(BATCH_SIZE).repeat(EPOCHS)\n",
    "    else:\n",
    "        batched_train_data = batched_train_data.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat(EPOCHS)\n",
    "\n",
    "    batched_val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE).repeat(EPOCHS)\n",
    "    batched_test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1) # batch size of 1, no repeat\n",
    "    batched_plot_data = tf.data.Dataset.from_tensor_slices((x_plot, y_plot)).batch(1)\n",
    "    return batched_train_data, batched_val_data, batched_test_data, batched_plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
