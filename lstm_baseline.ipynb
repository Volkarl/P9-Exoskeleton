{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions\n",
    "\n",
    "# Data layout in the xlsx files\n",
    "columns_data = ['1' ,'2', '3', '4', '5', '6', '7', '8', 'N/A_1', 'N/A_2', 'angle', 'time', 'session']\n",
    "columns_features_considered = columns_data[:8]\n",
    "column_ground_truth = columns_data[10]\n",
    "# Note that we ignore the 'time' column. That makes our data slightly imprecise as there are tiny, \n",
    "# TINY differences in time intervals in the real data (not worth modeling). Each timestep represents \n",
    "# 1 millisecond, 0.001 second. \n",
    "\n",
    "# Plot layout\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "# TODO: Try without this, see what happens?\n",
    "\n",
    "def plot_to_file(plot, ATTEMPT_NAME, TITLE):\n",
    "    plot.savefig(f'{ATTEMPT_NAME}/{TITLE}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY, \n",
    "            STEP_SIZE_SLIDING_WINDOW, PAST_HISTORY, \n",
    "            FUTURE_TARGET, VAL_PERCENT, TEST_PERCENT, \n",
    "            EPOCHS, BATCH_SPLITS, SMOOTHING, \n",
    "            ATTEMPT_NAME):\n",
    "    raw_data = load_dataset(DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY)\n",
    "    indexes, features, ground_truth = split_data(raw_data, GRANULARITY, SMOOTHING)\n",
    "    plot_dataset(features, ground_truth, ATTEMPT_NAME)\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = slice_data(indexes, features, ground_truth, VAL_PERCENT, \n",
    "                                                                TEST_PERCENT, PAST_HISTORY, FUTURE_TARGET, \n",
    "                                                                STEP_SIZE_SLIDING_WINDOW, GRANULARITY)\n",
    "    batched_train_data, batched_val_data, batched_val_data = batch_data(x_train, y_train, x_val, y_val, \n",
    "                                                                        x_test, y_test, BATCH_SPLITS, EPOCHS)\n",
    "    model = compile_model(x_train, FUTURE_TARGET)\n",
    "    training_history = fit_model(model, x_train, x_val, batched_train_data, batched_val_data, BATCH_SPLITS, EPOCHS)\n",
    "    plot_results(training_history, model, batched_val_data, indexes, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY):\n",
    "    # Read sheet 1 (table of contents), find index of entry with correct title, then load the corresponding excel sheet\n",
    "    table_of_contents = pd.read_excel(DATASET_FILE_PATH, sheet_name=0, header=None)\n",
    "    sheet_index = table_of_contents[table_of_contents[0] == f\"{DATASET_SHEET_TITLE}_raw_data\"][0].index[0]\n",
    "    sheet_data = pd.read_excel(DATASET_FILE_PATH, sheet_name=sheet_index + 1, header=None)\n",
    "    sheet_data.columns = columns_data\n",
    "    return sheet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(raw_data, GRANULARITY, SMOOTHING):\n",
    "    indexes = range(0, len(raw_data), 1)[::GRANULARITY] # Each timestep is a millisecond\n",
    "    features = raw_data[columns_features_considered][::GRANULARITY].ewm(span=SMOOTHING).mean()\n",
    "    ground_truth = pd.DataFrame(raw_data[column_ground_truth][::GRANULARITY]).ewm(span=SMOOTHING).mean()\n",
    "    return indexes, features, ground_truth\n",
    "    \n",
    "    # PERHAPS TRY THIS?\n",
    "    #data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\n",
    "    #data_std = dataset[:TRAIN_SPLIT].std(axis=0)\n",
    "    #dataset = (dataset-data_mean)/data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(features, ground_truth, ATTEMPT_NAME):\n",
    "    # plot_to_file(features.plot(subplots=True), ATTEMPT_NAME, \"features\"):\n",
    "    # plot_to_file(ground_truth.plot(), ATTEMPT_NAME, \"ground_truth\"):\n",
    "    features.plot(subplots=True)\n",
    "    ground_truth.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of all sliding windows of the data\n",
    "def multivariate_data(dataset_features, dataset_ground_truth, start_index, end_index, history_size,\n",
    "                      target_size, step, granularity, single_step=False, print_index=False):\n",
    "    data, labels = [], []\n",
    "    start_index = start_index + history_size \n",
    "    if end_index is None:\n",
    "        end_index = len(dataset_features) - target_size \n",
    "    if print_index: print(\"start\")\n",
    "    for i in range(start_index, end_index): # start 100, end 790. \n",
    "        if print_index: print(\"A\", i,)\n",
    "        indices = range(i-history_size, i, step) # range(0, 100) step size of 1          --- our sliding window\n",
    "        data.append(dataset_features[indices]) # append new array that contains all values within our sliding window\n",
    "        if single_step:\n",
    "            labels.append(dataset_ground_truth[i+target_size])\n",
    "        else:\n",
    "            labels.append(dataset_ground_truth[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "def slice_data(indexes, features, ground_truth, VAL_PERCENT, TEST_PERCENT, PAST_HISTORY, FUTURE_TARGET, \n",
    "               STEP_SIZE_SLIDING_WINDOW, GRANULARITY):\n",
    "\n",
    "    dataset = features.values\n",
    "    observations = len(dataset)\n",
    "    train_split = int(observations * (1 - VAL_PERCENT - TEST_PERCENT))\n",
    "    val_split = int(observations * (1 - VAL_PERCENT))\n",
    "    \n",
    "    plt.plot(indexes, dataset)\n",
    "    # savefig(f'{ATTEMPT_NAME}/dataset.png', bbox_inches='tight')\n",
    "\n",
    "    \n",
    "    \n",
    "    x_train, y_train = multivariate_data(dataset, ground_truth.values, 0,\n",
    "                                         train_split, PAST_HISTORY, FUTURE_TARGET, \n",
    "                                         STEP_SIZE_SLIDING_WINDOW, GRANULARITY, single_step = False, \n",
    "                                         print_index = False)\n",
    "    x_val, y_val = multivariate_data(dataset, ground_truth.values, train_split, \n",
    "                                         val_split, PAST_HISTORY, FUTURE_TARGET, \n",
    "                                         STEP_SIZE_SLIDING_WINDOW, GRANULARITY, single_step=False, \n",
    "                                         print_index = False)\n",
    "    x_test, y_test = multivariate_data(dataset, ground_truth.values, val_split, \n",
    "                                         None, PAST_HISTORY, FUTURE_TARGET, \n",
    "                                         STEP_SIZE_SLIDING_WINDOW, GRANULARITY, single_step=False, \n",
    "                                         print_index = False)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(x_train, y_train, x_val, y_val, x_test, y_test, BATCH_SPLITS, EPOCHS):\n",
    "    batched_train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(BATCH_SPLITS).repeat(EPOCHS)\n",
    "    batched_val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SPLITS).repeat(EPOCHS)\n",
    "    batched_test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SPLITS) # no repeat\n",
    "    return batched_train_data, batched_val_data, batched_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(x_train, FUTURE_TARGET):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(32, input_shape=x_train.shape[-2:], return_sequences = False))\n",
    "    model.add(tf.keras.layers.Dense(FUTURE_TARGET))\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae',metrics=['categorical_accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, x_train, x_val, batched_train_data, batched_val_data, BATCH_SPLITS, EPOCHS):\n",
    "    eval_interval = len(x_train) // BATCH_SPLITS \n",
    "    val_steps = len(x_val) // BATCH_SPLITS\n",
    "    # TODO: WHAT DO THESE TWO VALUES TRULY MEAN? ARE THEY WORTH EXPERIMENTING WITH?\n",
    "\n",
    "    training_history = model.fit(batched_train_data, epochs=EPOCHS,\n",
    "                        steps_per_epoch=eval_interval,\n",
    "                        validation_data=batched_val_data,\n",
    "                        validation_steps=val_steps)\n",
    "    return training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_history(history, title):\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  epochs = range(len(loss))\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "  plt.title(title)\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def plot_all(trained_model, batched_val_data, indexes, ground_truth):\n",
    "    predictions = [trained_model.predict(elem)[0] for elem in batched_val_data]\n",
    "    pred_size = len(indexes) - len(predictions)\n",
    "    plt.plot(indexes[pred_size:], predictions, 'r')\n",
    "    plt.plot(indexes[pred_size:], ground_truth[pred_size:], 'b')\n",
    "# Todo: use separate test-data instead of using validation data for testing\n",
    "\n",
    "def plot_results(training_history, trained_model, batched_val_data, indexes, ground_truth):\n",
    "    plot_train_history(training_history, 'Multi-Step Training and validation loss')\n",
    "    # savefig(f'{ATTEMPT_NAME}/features.png', bbox_inches='tight')\n",
    "    plot_all(trained_model, batched_val_data, indexes, ground_truth)\n",
    "    # savefig(f'{ATTEMPT_NAME}/features.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
