{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "# DATASET_FILE_PATH: \"BigData.xlsx\",\n",
    "# DATASET_SHEET_TITLE: \"data_test\",\n",
    "# GRANULARITY: 10, # Take first item, skip next x, take next, skip x, take next, ...\n",
    "# STEP_SIZE_SLIDING_WINDOW: 5,\n",
    "# PAST_HISTORY: 20,\n",
    "# FUTURE_TARGET: 1, \n",
    "# Looks at observations that are PASTHISTORY timesteps prior (reading only every STEP_SIZE_SLIDING_WINDOW entry),\n",
    "# then predicts the result at FUTURETARGET timesteps in the future\n",
    "# VAL_PERCENT: 0.1,\n",
    "# TEST_PERCENT: 0.1,\n",
    "# EPOCHS: 5,\n",
    "# BATCH_SIZE: 10 # Splits the dataset into batches of this size: we perform gradiant descent once per batch\n",
    "# SMOOTHING: 0\n",
    "\n",
    "# runAll(DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY, STEP_SIZE_SLIDING_WINDOW, PAST_HISTORY, \n",
    "#        FUTURE_TARGET, VAL_PERCENT, TEST_PERCENT, EPOCHS, BATCH_SPLITS_TRAIN, BATCH_SPLITS_VAL, SMOOTHING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load algorithm\n",
    "# TODO?: In the future I might want to split this up further into more files\n",
    "%run ./lstm_baseline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILE_PATH=\"Datasets\\\\BigData.xlsx\"\n",
    "DATASET_SHEET_TITLE=\"data_test\"\n",
    "GRANULARITY=10\n",
    "STEP_SIZE_SLIDING_WINDOW=5\n",
    "PAST_HISTORY=20\n",
    "FUTURE_TARGET=1\n",
    "VAL_PERCENT=0.1\n",
    "TEST_PERCENT=0.1\n",
    "EPOCHS=5\n",
    "BATCH_SPLITS=10\n",
    "SMOOTHING=50\n",
    "ATTEMPT_NAME=\"LSTM_BASELINE\"\n",
    "SHUFFLE_BUFFER_SIZE=100\n",
    "\n",
    "# Running the algorithm all at once\n",
    "BASELINE = run_all(DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY, STEP_SIZE_SLIDING_WINDOW, \n",
    "                   PAST_HISTORY, FUTURE_TARGET, VAL_PERCENT, TEST_PERCENT, EPOCHS, BATCH_SPLITS, \n",
    "                   SMOOTHING, ATTEMPT_NAME, SHUFFLE_BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the algorithm part-by-part\n",
    "raw_data = load_dataset(DATASET_FILE_PATH, DATASET_SHEET_TITLE, GRANULARITY)\n",
    "indexes, features, ground_truth = split_data(raw_data, GRANULARITY, SMOOTHING)\n",
    "plot_dataset(features, ground_truth, ATTEMPT_NAME)\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = slice_data(indexes, features, ground_truth, VAL_PERCENT, \n",
    "                                                            TEST_PERCENT, PAST_HISTORY, FUTURE_TARGET, \n",
    "                                                            STEP_SIZE_SLIDING_WINDOW, GRANULARITY)\n",
    "batched_train_data, batched_val_data, batched_test_data = batch_data(x_train, y_train, x_val, y_val, \n",
    "                                                                     x_test, y_test, BATCH_SPLITS, EPOCHS,\n",
    "                                                                     SHUFFLE_BUFFER_SIZE)\n",
    "model = compile_model(x_train, FUTURE_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = fit_model(model, x_train, x_val, batched_train_data, batched_val_data, BATCH_SPLITS, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(training_history, model, batched_test_data, indexes, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, batched_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUTURE WORKS:\n",
    "# - Modify it such that I can import different sorts of MI algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
